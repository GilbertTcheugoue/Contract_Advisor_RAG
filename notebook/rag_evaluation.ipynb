{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ1ExPp5LEfY"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation with RAGAS and Advanced Retrieval Methods Using LangChain**"
      ],
      "metadata": {
        "id": "8h8pkWXeS_mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken"
      ],
      "metadata": {
        "id": "wbgAPymFTDgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "w6FCZBTbTOkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection**\n",
        "\n",
        "we will use legal contract provided as context."
      ],
      "metadata": {
        "id": "4WFeGo55TQA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"../data/Raptor Contract.pdf\")\n",
        "base_docs = loader.load()"
      ],
      "metadata": {
        "id": "2BWbHQZ-TiOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in base_docs:\n",
        "  print(doc.metadata)"
      ],
      "metadata": {
        "id": "4mmHgacsTsQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating an Index**\n",
        "\n",
        "Let's use a naive index creation strategy of just using RecursiveCharacterTextSplitter on our documents and embedding each into our VectorStore using OpenAIEmbeddings().\n",
        "\n",
        "RecursiveCharacterTextSplitter()\n",
        "\n",
        "Chroma\n",
        "\n",
        "OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "Te0XyuY3T2XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
        "\n",
        "docs = text_splitter.split_documents(base_docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "LI2OBBxET7p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "mOqXsNVyUHNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max([len(chunk.page_content) for chunk in docs]))"
      ],
      "metadata": {
        "id": "NwueQQ41UIe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content"
      ],
      "metadata": {
        "id": "ki4KZ0X9UNfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert our Chroma vectorstore into a retriever with the .as_retriever() method."
      ],
      "metadata": {
        "id": "LdmEGeumUdOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ],
      "metadata": {
        "id": "AA7ff7ayUfNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs = base_retriever.get_relevant_documents(\"How much is the escrow amount?\")"
      ],
      "metadata": {
        "id": "zZu6qoNPUrGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(relevant_docs)"
      ],
      "metadata": {
        "id": "rMLXl7nyUsLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs[1]"
      ],
      "metadata": {
        "id": "Ps019GsiUxgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Retrieval Augmented Generation Prompt**"
      ],
      "metadata": {
        "id": "oXkVwkvSU-HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "sIXoNlmgVBoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up  Basic QA Chain**"
      ],
      "metadata": {
        "id": "5FbZFufuVF7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "5w_U0qzwVPh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much is the retention amount?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZhsIDgg0VdD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ground Truth Dataset Creation Using GPT-3.5-turbo and GPT-4**\n",
        "\n",
        "The next section might take you a long time to run, so the evaluation dataset is provided.\n",
        "\n",
        "The basic idea is that we can use LangChain to create questions based on our contexts, and then answer those questions.\n",
        "\n",
        "Let's look at how that works in the code!"
      ],
      "metadata": {
        "id": "HnICp7-aVkQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "question_schema = ResponseSchema(\n",
        "    name=\"question\",\n",
        "    description=\"a question about the context.\"\n",
        ")\n",
        "\n",
        "question_response_schemas = [\n",
        "    question_schema,\n",
        "]"
      ],
      "metadata": {
        "id": "OmhG7iwAVo4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
        "format_instructions = question_output_parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "D4DG6cAPURb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
        "\n",
        "bare_prompt_template = \"{content}\"\n",
        "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
      ],
      "metadata": {
        "id": "G439Sz2mV1Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
        "\n",
        "question: a question about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "question\n",
        "\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=docs[0],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "question_generation_chain = bare_template | question_generation_llm\n",
        "\n",
        "response = question_generation_chain.invoke({\"content\" : messages})\n",
        "output_dict = question_output_parser.parse(response.content)"
      ],
      "metadata": {
        "id": "VUIEFpwqV83G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in output_dict.items():\n",
        "  print(k)\n",
        "  print(v)"
      ],
      "metadata": {
        "id": "rIBalw99WCdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U tqdm"
      ],
      "metadata": {
        "id": "XLLPUIvLWGCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "qac_triples = []\n",
        "\n",
        "for text in tqdm(docs[:10]):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=text,\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = question_generation_chain.invoke({\"content\" : messages})\n",
        "  try:\n",
        "    output_dict = question_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  output_dict[\"context\"] = text\n",
        "  qac_triples.append(output_dict)"
      ],
      "metadata": {
        "id": "qMXxHbL4WQkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qac_triples[5]"
      ],
      "metadata": {
        "id": "bWMbCAvmWZYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_generation_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
        "\n",
        "answer_schema = ResponseSchema(\n",
        "    name=\"answer\",\n",
        "    description=\"an answer to the question\"\n",
        ")\n",
        "\n",
        "answer_response_schemas = [\n",
        "    answer_schema,\n",
        "]\n",
        "\n",
        "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
        "format_instructions = answer_output_parser.get_format_instructions()\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
        "\n",
        "answer: a answer about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "answer\n",
        "\n",
        "question: {question}\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=qac_triples[0][\"context\"],\n",
        "    question=qac_triples[0][\"question\"],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "answer_generation_chain = bare_template | answer_generation_llm\n",
        "\n",
        "response = answer_generation_chain.invoke({\"content\" : messages})\n",
        "output_dict = answer_output_parser.parse(response.content)"
      ],
      "metadata": {
        "id": "htE7DUXWWbI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in output_dict.items():\n",
        "  print(k)\n",
        "  print(v)"
      ],
      "metadata": {
        "id": "osKHoe_XWqrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for triple in tqdm(qac_triples):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=triple[\"context\"],\n",
        "      question=triple[\"question\"],\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
        "  try:\n",
        "    output_dict = answer_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  triple[\"answer\"] = output_dict[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "nsE9KHtkWuMm",
        "outputId": "366b306b-6a6b-476d-f3a5-fbb4f5e69ff0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tqdm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-16b7d7f195f4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqac_triples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   messages = prompt_template.format_messages(\n\u001b[1;32m      3\u001b[0m       \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mformat_instructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_instructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U datasets"
      ],
      "metadata": {
        "id": "O1gemP3sW0b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
        "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
        "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})"
      ],
      "metadata": {
        "id": "x1gEMl-AW48I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
      ],
      "metadata": {
        "id": "_4LL1N8BXHXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "id": "yMoYNt9dXNX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[3]"
      ],
      "metadata": {
        "id": "gCEXop8_XOoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")"
      ],
      "metadata": {
        "id": "HHHg70P2XUqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating RAG Pipelines**"
      ],
      "metadata": {
        "id": "e7DAdvgXXZSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Using RAGAS\n",
        "\n",
        "Now we can evaluate using RAGAS!"
      ],
      "metadata": {
        "id": "JNs_cQUIXkOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ragas"
      ],
      "metadata": {
        "id": "KZ82uM_sX0Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    context_relevancy,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "\n",
        "from ragas.metrics.critique import harmfulness\n",
        "from ragas import evaluate\n",
        "\n",
        "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"response\"].content,\n",
        "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset\n",
        "\n",
        "def evaluate_ragas_dataset(ragas_dataset):\n",
        "  result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "        context_relevancy,\n",
        "        answer_correctness,\n",
        "        answer_similarity\n",
        "    ],\n",
        "  )\n",
        "  return result"
      ],
      "metadata": {
        "id": "HL16iqMoX4Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)"
      ],
      "metadata": {
        "id": "vvltFI0jYI61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save data"
      ],
      "metadata": {
        "id": "HezrKm-5Ygxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_ragas_dataset.to_csv(\"basic_qa_ragas_dataset.csv\")"
      ],
      "metadata": {
        "id": "MbldTo0yYMmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)"
      ],
      "metadata": {
        "id": "BtMVwMykYaqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_qa_result"
      ],
      "metadata": {
        "id": "k67yS61uYqss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Other Retrievers**\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our RAGAS evaluation!"
      ],
      "metadata": {
        "id": "EzHqU1SoZVLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "  created_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"context\")\n",
        "      )\n",
        "    | {\n",
        "         \"response\": prompt | primary_qa_llm,\n",
        "         \"context\": itemgetter(\"context\"),\n",
        "      }\n",
        "  )\n",
        "\n",
        "  return created_qa_chain"
      ],
      "metadata": {
        "id": "oqYc0nebZYmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parent Document Retriever"
      ],
      "metadata": {
        "id": "9dlHVVikZld_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500)\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "\n",
        "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "store = InMemoryStore()"
      ],
      "metadata": {
        "id": "DJC9Lq22Zm6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ],
      "metadata": {
        "id": "qCQasNtpZ0tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retriever.add_documents(base_docs)"
      ],
      "metadata": {
        "id": "249hb-pWZ8Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ],
      "metadata": {
        "id": "d99ik4l3a7Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
      ],
      "metadata": {
        "id": "eNChPLVia9TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is the retention amount\"})[\"response\"].content\n"
      ],
      "metadata": {
        "id": "uwlCd7vMZ9Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is the retention amount\"})[\"response\"].content"
      ],
      "metadata": {
        "id": "62coSzD2bNf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdr_qa_ragas_dataset.to_csv(\"pdr_qa_ragas_dataset.csv\")"
      ],
      "metadata": {
        "id": "Rhw-q_78bRQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxDPmD_CY2-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}